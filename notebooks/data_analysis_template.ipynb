{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bbfd651",
   "metadata": {},
   "source": [
    "# 1. Import Libraries\n",
    "This notebook provides a generic template for data analysis and modeling. Adjust steps as needed for your specific dataset and task (regression or classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa9da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, classification_report, mean_squared_error, r2_score)\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "# Plot style\n",
    "sns.set_theme(context='notebook', style='whitegrid')\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed3317d",
   "metadata": {},
   "source": [
    "# 2. Load Dataset\n",
    "Load a dataset from CSV (or adapt to read from a database / API). Update `DATA_PATH` as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a8ecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this path or replace with DB read logic\n",
    "DATA_PATH = os.getenv('DATA_PATH', 'data_extracts/stocks/stocks_sample.csv')\n",
    "\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(f\"WARNING: {DATA_PATH} not found. Provide a valid CSV path.\")\n",
    "    df = pd.DataFrame()\n",
    "else:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print('Loaded dataset:', DATA_PATH)\n",
    "\n",
    "print('Shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54731e87",
   "metadata": {},
   "source": [
    "# 3. Inspect Raw Data\n",
    "Basic structure, dtypes, and initial statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb96381",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df.empty:\n",
    "    print('DataFrame empty; skip inspection.')\n",
    "else:\n",
    "    display(df.head())\n",
    "    print('\\nDtypes:')\n",
    "    print(df.dtypes)\n",
    "    print('\\nDescribe (numeric):')\n",
    "    display(df.describe().T)\n",
    "    # Categorical quick value counts (top 10)\n",
    "    cat_cols = [c for c in df.columns if df[c].dtype == 'object' and df[c].nunique() < 50]\n",
    "    for c in cat_cols:\n",
    "        print(f'\\nValue counts for {c}:')\n",
    "        print(df[c].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade92606",
   "metadata": {},
   "source": [
    "# 4. Data Cleaning\n",
    "Handle missing values, duplicates, and simple outlier flagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ded7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df.empty:\n",
    "    print('Skip cleaning (empty df).')\n",
    "else:\n",
    "    # Example: drop duplicate rows\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    print(f'Removed {before - len(df)} duplicate rows.')\n",
    "\n",
    "    # Example: simple missing value handling\n",
    "    missing_ratio = df.isna().mean()\n",
    "    cols_drop = [c for c,r in missing_ratio.items() if r > 0.4]\n",
    "    if cols_drop:\n",
    "        print('Dropping high-missing columns:', cols_drop)\n",
    "        df = df.drop(columns=cols_drop)\n",
    "    # Fill remaining numeric NaNs with median\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "\n",
    "    # Quick outlier flag (z-score > 4) for numeric columns\n",
    "    from scipy import stats\n",
    "    zscores = np.abs(stats.zscore(df[num_cols], nan_policy='omit'))\n",
    "    if isinstance(zscores, np.ndarray):\n",
    "        extreme_mask = (zscores > 4).any(axis=1)\n",
    "        print('Extreme outlier rows:', extreme_mask.sum())\n",
    "\n",
    "    print('Post-cleaning shape:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1733321",
   "metadata": {},
   "source": [
    "# 5. Feature Engineering\n",
    "Create/encode features and prepare data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75bce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = None  # Set to your target column name if supervised learning\n",
    "\n",
    "if df.empty:\n",
    "    print('Skip feature engineering (empty df).')\n",
    "else:\n",
    "    # Example: create simple numeric ratios if price columns present\n",
    "    if {'high','low','close'}.issubset(df.columns):\n",
    "        df['hl_range'] = df['high'] - df['low']\n",
    "        df['close_to_high_pct'] = (df['high'] - df['close']) / df['high'] * 100\n",
    "\n",
    "    # Identify categorical and numeric columns\n",
    "    categorical_cols = [c for c in df.columns if df[c].dtype == 'object']\n",
    "    numeric_cols = [c for c in df.columns if c not in categorical_cols]\n",
    "    if target_column and target_column in categorical_cols:\n",
    "        categorical_cols.remove(target_column)\n",
    "    if target_column and target_column in numeric_cols:\n",
    "        numeric_cols.remove(target_column)\n",
    "\n",
    "    print('Categorical:', categorical_cols)\n",
    "    print('Numeric:', numeric_cols[:10], '...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a66929",
   "metadata": {},
   "source": [
    "# 6. Train / Test Split\n",
    "We detect if the task looks like classification (few unique values) or regression and split accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14571d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "is_classification = False\n",
    "if target_column and target_column in df.columns:\n",
    "    target_series = df[target_column]\n",
    "    # Heuristic: classification if <= 15 unique values and not purely numeric continuous spread\n",
    "    if target_series.nunique() <= 15:\n",
    "        is_classification = True\n",
    "else:\n",
    "    print('No target_column set; subsequent modeling sections will be skipped unless you define one.')\n",
    "\n",
    "if target_column:\n",
    "    feature_cols = [c for c in df.columns if c != target_column]\n",
    "    X = df[feature_cols].select_dtypes(include=['number']).fillna(0)\n",
    "    y = df[target_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=not bool(df.index.is_monotonic_increasing), random_state=42)\n",
    "    print('Train shape:', X_train.shape, 'Test shape:', X_test.shape, 'Classification?' , is_classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb53a817",
   "metadata": {},
   "source": [
    "# 7. Baseline Model\n",
    "Simple baseline (LinearRegression or LogisticRegression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ba267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "baseline_model = None\n",
    "baseline_metrics = {}\n",
    "\n",
    "if target_column and target_column in df.columns:\n",
    "    if is_classification:\n",
    "        # Basic encoding for classification (ensure y is categorical/integer)\n",
    "        y_enc = y\n",
    "        if y.dtype == 'object':\n",
    "            y_enc = y.astype('category').cat.codes\n",
    "        baseline_model = LogisticRegression(max_iter=500)\n",
    "        baseline_model.fit(X_train, y_train if y.dtype != 'object' else y_enc.iloc[y_train.index])\n",
    "        y_pred = baseline_model.predict(X_test)\n",
    "        baseline_metrics['accuracy'] = accuracy_score(y_test if y.dtype != 'object' else y_enc.iloc[y_test.index], y_pred)\n",
    "        baseline_metrics['f1_macro'] = f1_score(y_test if y.dtype != 'object' else y_enc.iloc[y_test.index], y_pred, average='macro')\n",
    "    else:\n",
    "        baseline_model = LinearRegression()\n",
    "        baseline_model.fit(X_train, y_train)\n",
    "        y_pred = baseline_model.predict(X_test)\n",
    "        baseline_metrics['mae'] = mean_absolute_error(y_test, y_pred)\n",
    "        baseline_metrics['rmse'] = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print('Baseline metrics:', baseline_metrics if baseline_metrics else 'No target / modeling skipped')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbfb903",
   "metadata": {},
   "source": [
    "# 8. Advanced Model & Hyperparameter Tuning\n",
    "Grid search over RandomForest for improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67701cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gs_best = None\n",
    "gs_metrics = {}\n",
    "\n",
    "if target_column and target_column in df.columns:\n",
    "    if is_classification:\n",
    "        model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "        param_grid = {'n_estimators': [50, 100], 'max_depth': [5, None]}\n",
    "    else:\n",
    "        model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "        param_grid = {'n_estimators': [100, 200], 'max_depth': [5, None]}\n",
    "\n",
    "    grid = GridSearchCV(model, param_grid, cv=3, scoring='accuracy' if is_classification else 'neg_root_mean_squared_error', n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    gs_best = grid.best_estimator_\n",
    "    y_pred = gs_best.predict(X_test)\n",
    "    if is_classification:\n",
    "        gs_metrics['accuracy'] = accuracy_score(y_test, y_pred)\n",
    "        gs_metrics['f1_macro'] = f1_score(y_test, y_pred, average='macro')\n",
    "    else:\n",
    "        gs_metrics['rmse'] = mean_squared_error(y_test, y_pred, squared=False)\n",
    "        gs_metrics['mae'] = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print('GridSearch best params:' , (grid.best_params_ if gs_metrics else 'N/A'))\n",
    "print('GridSearch metrics:', gs_metrics if gs_metrics else 'Skipped')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f192ff64",
   "metadata": {},
   "source": [
    "# 9. Visualization of Results\n",
    "Feature importances, residuals (regression) or confusion matrix (classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db957e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "if gs_best is not None:\n",
    "    if is_classification:\n",
    "        y_pred = gs_best.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(5,4))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "    else:\n",
    "        y_pred = gs_best.predict(X_test)\n",
    "        residuals = y_test - y_pred\n",
    "        plt.figure(figsize=(5,4))\n",
    "        sns.scatterplot(x=y_pred, y=residuals, s=20)\n",
    "        plt.axhline(0, color='red', linestyle='--')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Residuals')\n",
    "        plt.title('Residual Plot')\n",
    "        plt.show()\n",
    "\n",
    "    # Feature importance if available\n",
    "    if hasattr(gs_best, 'feature_importances_'):\n",
    "        importances = gs_best.feature_importances_\n",
    "        fi_df = (pd.DataFrame({'feature': X_train.columns, 'importance': importances})\n",
    "                    .sort_values('importance', ascending=False).head(20))\n",
    "        plt.figure(figsize=(6,6))\n",
    "        sns.barplot(data=fi_df, x='importance', y='feature')\n",
    "        plt.title('Top Feature Importances')\n",
    "        plt.show()\n",
    "else:\n",
    "    print('Skip visualization (no tuned model).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17f496a",
   "metadata": {},
   "source": [
    "# 10. Save Model & Artifacts\n",
    "Persist the tuned model and optionally export processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib, os, datetime as dt\n",
    "\n",
    "artifacts_dir = 'artifacts'\n",
    "os.makedirs(artifacts_dir, exist_ok=True)\n",
    "\n",
    "timestamp = dt.datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
    "if gs_best is not None:\n",
    "    model_path = os.path.join(artifacts_dir, f'model_{timestamp}.joblib')\n",
    "    joblib.dump(gs_best, model_path)\n",
    "    print('Saved model to', model_path)\n",
    "else:\n",
    "    print('No tuned model to save.')\n",
    "\n",
    "processed_path = os.path.join(artifacts_dir, f'processed_{timestamp}.csv')\n",
    "if not df.empty:\n",
    "    df.to_csv(processed_path, index=True)\n",
    "    print('Saved processed data to', processed_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1218748e",
   "metadata": {},
   "source": [
    "# 11. Next Steps\n",
    "- Refine feature engineering (technical indicators, rolling stats)\n",
    "- Integrate with live pipeline outputs\n",
    "- Add model monitoring (drift, performance decay)\n",
    "- Automate retraining schedule\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
